library(rpart)
library(rpart.plot)
library(dplyr)
library(ggplot2)
library(randomForest)
library(caTools)
library(mlbench)
library(caret)

getwd()
setwd("C:/Users/v063019/Desktop/Analytics")


set.seed(101)
dataset = read.csv('Data.csv')

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')

library(caTools)
set.seed(123)
split = sample.split(dataset$DependentVariable, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

-----------------------------------------------------------------------------------------

# Taking care of missing data
dataset$Age = ifelse(is.na(dataset$Age),
                     ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
                     dataset$Age)

-----------------------------------------------------------------------------------------

# Encoding categorical data
dataset$Country = factor(dataset$Country,
                         levels = c('France', 'Spain', 'Germany'),
                         labels = c(1, 2, 3))
						 
------------------------------------------------------------------------------------------

# Feature Scaling
training_set[,2:3] = scale(training_set[,2:3])
test_set[,2:3] = scale(test_set[,2:3])

------------------------------------------------------------------------------------------

# Fitting Simple Linear Regression to the Training set
regressor = lm(formula = Salary ~ YearsExperience,
               data = training_set)

------------------------------------------------------------------------------------------	
# Predicting the Test set results
y_pred = predict(regressor, newdata = test_set)

------------------------------------------------------------------------------------------	

# Function that returns Root Mean Squared Error
rmse <- function(error)
{
  sqrt(mean(error^2))
}

# Function that returns Mean Absolute Error
mae <- function(error)
{
  mean(abs(error))
}

#r2

R2=function(actual,predict)
{
  R2 <- 1 - (sum((actual-predict )^2)/sum((actual-mean(actual))^2))
  print(R2)
}

R2=function(pred, obs, formula = "corr", na.rm = FALSE) {
  n <- sum(complete.cases(pred))
  switch(formula,
         corr = cor(obs, pred, use = ifelse(na.rm, "complete.obs", "everything"))^2,
         traditional = 1 - (sum((obs-pred)^2, na.rm = na.rm)/((n-1)*var(obs, na.rm = na.rm))))
}

# Calculate error
error=pred-test$Salary
rmse(error)
mae(error)
R2(pred,test$Salary) 

------------------------------------------------------------------------------------------	

# Fitting Multiple Linear Regression to the Training set
regressor = lm(formula = Profit ~ .,
               data = training_set)

# Predicting the Test set results
y_pred = predict(regressor, newdata = test_set)

------------------------------------------------------------------------------------------	
# Fitting Linear Regression to the dataset
lin_reg = lm(formula = Salary ~ .,
             data = dataset)

# Fitting Polynomial Regression to the dataset
dataset$Level2 = dataset$Level^2
dataset$Level3 = dataset$Level^3
dataset$Level4 = dataset$Level^4
poly_reg = lm(formula = Salary ~ .,
              data = dataset)

------------------------------------------------------------------------------------------	
# Fitting SVR to the dataset
library(e1071)
regressor = svm(formula = Salary ~ .,
                data = dataset,
                type = 'eps-regression',
                kernel = 'radial')

# Predicting a new result
y_pred = predict(regressor, data.frame(Level = 6.5))	

------------------------------------------------------------------------------------------	
# Fitting Decision Tree to the dataset
library(rpart)
regressor = rpart(formula = Salary ~ .,
                  data = dataset,
                  control = rpart.control(minsplit = 1))

# Predicting a new result with Decision Tree Regression
y_pred = predict(regressor, data.frame(Level = 6.5))

------------------------------------------------------------------------------------------	
# Fitting Random Forest Regression to the dataset
library(randomForest)
set.seed(1234)
regressor = randomForest(x = dataset[-2],
                         y = dataset$Salary,
                         ntree = 500)

# Predicting a new result with Random Forest Regression
y_pred = predict(regressor, data.frame(Level = 6.5))		

------------------------------------------------------------------------------------------	  


-------------------------------------------------------------------------------------------------
# attach the BostonHousing dataset
data(BostonHousing)

# Split out validation dataset
# create a list of 80% of the rows in the original dataset we can use for training
set.seed(7)
validation_index <- createDataPartition(BostonHousing$medv, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- BostonHousing[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- BostonHousing[validation_index,]


# Run algorithms using 10-fold cross validation
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
# lm
set.seed(7)
model <- train(medv~., data=dataset, method="lm", metric=metric, preProc=c("center", "scale"), trControl=control)


# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)

-------------------------------------------------------------------------------------------------
#feature Elimination

#ensure the results are repeatable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# load the data
data(BostonHousing)
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(BostonHousing[,1:13], BostonHousing[,14], sizes=c(1:13), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))

---------------------------------------------------------------------------------------------------
#Remove Redundant Feature 

# load the data
data(BostonHousing)
# calculate correlation matrix
correlationMatrix <- cor(BostonHousing[,1:13])
#find type
sapply(BostonHousing,class)
# summarize the correlation matrix
print(correlationMatrix)

#to numeric
BostonHousing$chas=as.numeric(BostonHousing$chas)


# calculate correlation matrix
correlationMatrix <- cor(BostonHousing[,1:13])

library(corrplot)
corrplot(correlationMatrix, method="circle")

# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)

---------------------------------------------------------------------------------------------------
#Overall Regression Testing

# Housing Values in Suburbs of Boston

# Regression, numeric inputs

# Dataset Description: https://archive.ics.uci.edu/ml/datasets/Housing


# load libraries
library(mlbench)
library(caret)
library(corrplot)

# attach the BostonHousing dataset
data(BostonHousing)

# Split out validation dataset
# create a list of 80% of the rows in the original dataset we can use for training
set.seed(7)
validation_index <- createDataPartition(BostonHousing$medv, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- BostonHousing[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- BostonHousing[validation_index,]


# Summarize data

# dimensions of dataset
dim(dataset)

# list types for each attribute
sapply(dataset, class)

# take a peek at the first 5 rows of the data
head(dataset, n=20)

# summarize attribute distributions
summary(dataset)

# convert factor to numeric
dataset[,4] <- as.numeric(as.character(dataset[,4]))


# summarize correlations between input variables
cor(dataset[,1:13])


# Univaraite Visualization

# histograms each attribute
par(mfrow=c(2,7))
for(i in 1:13) {
	hist(dataset[,i], main=names(dataset)[i])
}

# density plot for each attribute
par(mfrow=c(2,7))
for(i in 1:13) {
	plot(density(dataset[,i]), main=names(dataset)[i])
}

# boxplots for each attribute
par(mfrow=c(2,7))
for(i in 1:13) {
	boxplot(dataset[,i], main=names(dataset)[i])
}


# Multivariate Visualizations

# scatterplot matrix
pairs(dataset[,1:13])

# correlation plot
correlations <- cor(dataset[,1:13])
corrplot(correlations, method="circle")


# Evaluate Algorithms: Baseline

# Run algorithms using 10-fold cross validation
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
# lm
set.seed(7)
fit.lm <- train(medv~., data=dataset, method="lm", metric=metric, preProc=c("center", "scale"), trControl=control)
# GLM
set.seed(7)
fit.glm <- train(medv~., data=dataset, method="glm", metric=metric, preProc=c("center", "scale"), trControl=control)
# GLMNET
set.seed(7)
fit.glmnet <- train(medv~., data=dataset, method="glmnet", metric=metric, preProc=c("center", "scale"), trControl=control)
# SVM
set.seed(7)
fit.svm <- train(medv~., data=dataset, method="svmRadial", metric=metric, preProc=c("center", "scale"), trControl=control)
# CART
set.seed(7)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart <- train(medv~., data=dataset, method="rpart", metric=metric, tuneGrid=grid, preProc=c("center", "scale"), trControl=control)
# kNN
set.seed(7)
fit.knn <- train(medv~., data=dataset, method="knn", metric=metric, preProc=c("center", "scale"), trControl=control)
# Compare algorithms
results <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet, SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
summary(results)
dotplot(results)


# Evaluate Algorithms: Feature Selection

# remove correlated attributes
# find attributes that are highly corrected
set.seed(7)
cutoff <- 0.70
correlations <- cor(dataset[,1:13])
highlyCorrelated <- findCorrelation(correlations, cutoff=cutoff)
for (value in highlyCorrelated) {
	print(names(dataset)[value])
}
# create a new dataset without highly corrected features
dataset_features <- dataset[,-highlyCorrelated]
dim(dataset_features)

# Run algorithms using 10-fold cross validation
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
# lm
set.seed(7)
fit.lm <- train(medv~., data=dataset_features, method="lm", metric=metric, preProc=c("center", "scale"), trControl=control)
# GLM
set.seed(7)
fit.glm <- train(medv~., data=dataset_features, method="glm", metric=metric, preProc=c("center", "scale"), trControl=control)
# GLMNET
set.seed(7)
fit.glmnet <- train(medv~., data=dataset_features, method="glmnet", metric=metric, preProc=c("center", "scale"), trControl=control)
# SVM
set.seed(7)
fit.svm <- train(medv~., data=dataset_features, method="svmRadial", metric=metric, preProc=c("center", "scale"), trControl=control)
# CART
set.seed(7)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart <- train(medv~., data=dataset_features, method="rpart", metric=metric, tuneGrid=grid, preProc=c("center", "scale"), trControl=control)
# kNN
set.seed(7)
fit.knn <- train(medv~., data=dataset_features, method="knn", metric=metric, preProc=c("center", "scale"), trControl=control)
# Compare algorithms
feature_results <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet, SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
summary(feature_results)
dotplot(feature_results)


# Evaluate Algorithnms: Box-Cox Transform

# Run algorithms using 10-fold cross validation
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
# lm
set.seed(7)
fit.lm <- train(medv~., data=dataset, method="lm", metric=metric, preProc=c("center", "scale", "BoxCox"), trControl=control)
# GLM
set.seed(7)
fit.glm <- train(medv~., data=dataset, method="glm", metric=metric, preProc=c("center", "scale", "BoxCox"), trControl=control)
# GLMNET
set.seed(7)
fit.glmnet <- train(medv~., data=dataset, method="glmnet", metric=metric, preProc=c("center", "scale", "BoxCox"), trControl=control)
# SVM
set.seed(7)
fit.svm <- train(medv~., data=dataset, method="svmRadial", metric=metric, preProc=c("center", "scale", "BoxCox"), trControl=control)
# CART
set.seed(7)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
fit.cart <- train(medv~., data=dataset, method="rpart", metric=metric, tuneGrid=grid, preProc=c("center", "scale", "BoxCox"), trControl=control)
# kNN
set.seed(7)
fit.knn <- train(medv~., data=dataset, method="knn", metric=metric, preProc=c("center", "scale", "BoxCox"), trControl=control)
# Compare algorithms
transform_results <- resamples(list(LM=fit.lm, GLM=fit.glm, GLMNET=fit.glmnet, SVM=fit.svm, CART=fit.cart, KNN=fit.knn))
summary(transform_results)
dotplot(transform_results)


# Improve Results With Tuning

# look at parameters
print(fit.svm)

# tune SVM sigma and C parametres
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
set.seed(7)
grid <- expand.grid(.sigma=c(0.025, 0.05, 0.1, 0.15), .C=seq(1, 10, by=1))
fit.svm <- train(medv~., data=dataset, method="svmRadial", metric=metric, tuneGrid=grid, preProc=c("BoxCox"), trControl=control)
print(fit.svm)
plot(fit.svm)


# Ensemble Methods
seed=7
# try ensembles
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
# Random Forest
set.seed(seed)
fit.rf <- train(medv~., data=dataset, method="rf", metric=metric, preProc=c("BoxCox"), trControl=control)
# Stochastic Gradient Boosting
set.seed(seed)
fit.gbm <- train(medv~., data=dataset, method="gbm", metric=metric, preProc=c("BoxCox"), trControl=control, verbose=FALSE)
# Cubist
set.seed(seed)
fit.cubist <- train(medv~., data=dataset, method="cubist", metric=metric, preProc=c("BoxCox"), trControl=control)
# Compare algorithms
ensemble_results <- resamples(list(RF=fit.rf, GBM=fit.gbm, CUBIST=fit.cubist))
summary(ensemble_results)
dotplot(ensemble_results)


# Tune Cubist

# look at parameters used for Cubist
print(fit.cubist)

# Tune the Cubist algorithm
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
set.seed(7)
grid <- expand.grid(.committees=seq(15, 25, by=1), .neighbors=c(3, 5, 7))
tune.cubist <- train(medv~., data=dataset, method="cubist", metric=metric, preProc=c("BoxCox"), tuneGrid=grid, trControl=control)
print(tune.cubist)
plot(tune.cubist)


# Finalize Model


# prepare the data transform using training data
set.seed(7)
x <- dataset[,1:13]
y <- dataset[,14]
preprocessParams <- preProcess(x, method=c("BoxCox"))
trans_x <- predict(preprocessParams, x)
# train the final model
finalModel <- cubist(x=trans_x, y=y, committees=18)
summary(finalModel)

# transform the validation dataset
set.seed(7)
val_x <- validation[,1:13]
trans_val_x <- predict(preprocessParams, val_x)
val_y <- validation[,14]
# use final model to make predictions on the validation dataset
predictions <- predict(finalModel, newdata=trans_val_x, neighbors=3)
# calculate RMSE
rmse <- RMSE(predictions, val_y)
r2 <- R2(predictions, val_y)
print(rmse)



# Visualising the Training set results
library(ggplot2)
ggplot() +
  geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary),
             colour = 'red') +
  geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)),
            colour = 'blue') +
  ggtitle('Salary vs Experience (Training set)') +
  xlab('Years of experience') +
  ylab('Salary')

------------------------------------------------------------------------------------------	

# Function that returns Root Mean Squared Error
rmse <- function(error)
{
  sqrt(mean(error^2))
}

# Function that returns Mean Absolute Error
mae <- function(error)
{
  mean(abs(error))
}

#r2

R2=function(actual,predict)
{
  R2 <- 1 - (sum((actual-predict )^2)/sum((actual-mean(actual))^2))
  print(R2)
}

R2=function(pred, obs, formula = "corr", na.rm = FALSE) {
  n <- sum(complete.cases(pred))
  switch(formula,
         corr = cor(obs, pred, use = ifelse(na.rm, "complete.obs", "everything"))^2,
         traditional = 1 - (sum((obs-pred)^2, na.rm = na.rm)/((n-1)*var(obs, na.rm = na.rm))))
}

# Calculate error
error=pred-test$Salary
rmse(error)
mae(error)
R2(pred,test$Salary) 

------------------------------------------------------------------------------------------	

# Fitting Multiple Linear Regression to the Training set
regressor = lm(formula = Profit ~ .,
               data = training_set)

# Predicting the Test set results
y_pred = predict(regressor, newdata = test_set)

------------------------------------------------------------------------------------------	
# Fitting Linear Regression to the dataset
lin_reg = lm(formula = Salary ~ .,
             data = dataset)

# Fitting Polynomial Regression to the dataset
dataset$Level2 = dataset$Level^2
dataset$Level3 = dataset$Level^3
dataset$Level4 = dataset$Level^4
poly_reg = lm(formula = Salary ~ .,
              data = dataset)

------------------------------------------------------------------------------------------	
# Fitting SVR to the dataset
library(e1071)
regressor = svm(formula = Salary ~ .,
                data = dataset,
                type = 'eps-regression',
                kernel = 'radial')

# Predicting a new result
y_pred = predict(regressor, data.frame(Level = 6.5))	

------------------------------------------------------------------------------------------	
# Fitting Decision Tree to the dataset
library(rpart)
regressor = rpart(formula = Salary ~ .,
                  data = dataset,
                  control = rpart.control(minsplit = 1))

# Predicting a new result with Decision Tree Regression
y_pred = predict(regressor, data.frame(Level = 6.5))

------------------------------------------------------------------------------------------	
# Fitting Random Forest Regression to the dataset
library(randomForest)
set.seed(1234)
regressor = randomForest(x = dataset[-2],
                         y = dataset$Salary,
                         ntree = 500)

# Predicting a new result with Random Forest Regression
y_pred = predict(regressor, data.frame(Level = 6.5))		

------------------------------------------------------------------------------------------	  

Net DM - ((netPrice-directCost)/netprice)*100
Net SM - ((netPrice-(directCost+sharedCost))/netprice)*100
List DM - ((listPrice-directCost)/listPrice)*100
List SM - ((listPrice-(directCost+sharedCost))/listPrice)*100

==============================================
=IF(AND(X2>=0,X2<5),"RANGE0-4%",IF(AND(X2>=5,X2<10),"RANGE5-9%",IF(AND(X2>=10,X2<20),"RANGE10-20%",IF(AND(X2>=20,X2<30),"RANGE20-30%",IF(AND(X2>=30,X2<40),"RANGE30-40%",IF(AND(X2>=40,X2<50),"RANGE40-50%",IF(AND(X2>=50,X2<60),"RANGE50-60%",IF(AND(X2>=60,X2<70),"RANGE60-70%",IF(AND(X2>=70,X2<80),"RANGE70-80%",IF(AND(X2>=80,X2<90),"RANGE80-90%",IF(AND(X2>=90,X2<100),"RANGE90-100%")))))))))))

=IF(AND(A2>=0,A2<10),"RANGE0-10%",IF(AND(A2>10,A2<=15),"RANGE10-15%",IF(AND(A2>=15,A2<20),"RANGE15-19%",IF(AND(A2>=20,A2<25),"RANGE20-24%",IF(AND(A2>=25,A2<30),"RANGE25-29%",IF(AND(A2>=30,A2<35),"RANGE30-34%",IF(AND(A2>=35,A2<40),"RANGE35-39%",IF(AND(A2>=40,A2<45),"RANGE40-44%",IF(AND(A2>=45,A2<50),"RANGE45-49%",IF(AND(A2>=50,A2<55),"RANGE50-54%",IF(AND(A2>=55,A2<60),"RANGE55-59%",IF(AND(A2>=60,A2<65),"RANGE60-64%",IF(AND(A2>=65,A2<70),"RANGE64-69%",IF(AND(A2>=70,A2<75),"RANGE70-74%",IF(AND(A2>=75,A2<80),"RANGE75-79%",IF(AND(A2>=80,A2<85),"RANGE80-84%",IF(AND(A2>=85,A2<90),"RANGE85-89%",IF(AND(A2>=90,A2<95),"RANGE90-95%"))))))))))))))))))

----------------
# Cubist
set.seed(7)
fit.cubist <- train(Discount_Percentage~DD_Total_List_Price+Term+DD_REC_List_Price+DD_NRE_List_Price
                    +DD_REC_Direct_Cost+DD_NRE_Direct_Cost+DD_REC_Shared_Cost+DD_NRE_Shared_Cost+
                      DD_List_Total_Direct_Margin_Percent+DD_List_Total_Shared_Margin_Percent, data=training_set, method="cubist", metric=metric, preProc=c("BoxCox"), trControl=control)

					  
# Scale data - used to scale the data to lower range ***** Z score Standardisation *********
scaled.data <- scale(final_data[ ,c(1,3:8)])


************************** Data normalization between 0 to 1 (Min - Max) ************ KNN scaling 


data_norm <- function(x) {((x - min(x))/ (max(x) - min(x)))}
pip_norm  <- as.data.frame(lapply(PIP_data1 , data_norm))

--------------------------------------------------------------------------------------------------------
t <- table(predictions = y_pred, actual = pip_data$Disc_Group)
sum(diag(t))/ sum(t)
---------------------------------------------------------------------------------------------------------------

install.packages("rpart.plot")
install.packages("rpart")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("randomForest")
install.packages("caTools")
install.packages("mlbench")
install.packages("caret")
install.packages("e1071")
install.packages("glmnet")
install.packages("xgboost")
install.packages("Cubist")
install.packages("party")
install.packages("sqldf")
install.packages("forecast")
install.packages("plyr")
install.packages("stringr")
install.packages("ggplot2")
install.packages("qcc")
install.packages("reshape2")

install.packages("dplyr")
install.packages("tidyr")
install.packages("stringr")
install.packages("lubridate")
install.packages("rgl")
install.packages("htmlwidgets")
install.packages("glmnet")
install.packages("kernlab")
install.packages("e1071")
install.packages("tree")
install.packages("mboost")
install.packages("car")
install.packages("gbm")
install.packages("rocr")

install.packages("MASS")
install.packages("caret")
install.packages(caTools)

===============================================

# save the model to disk
saveRDS(final_model, "./final_model.rds")
 
# later...
 
# load the model
super_model <- readRDS("./final_model.rds")
